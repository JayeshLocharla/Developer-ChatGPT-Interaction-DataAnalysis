{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "YNO9rAmjDEXh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f5a79be-2b99-4eec-91c4-c19da4388df6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NAIST-SE/DevGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTGtnMZTDH_M",
        "outputId": "fe8febe3-eff0-47c6-9dc5-266498f8b55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DevGPT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "lBZM78ykVMQb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69509a61-81e8-44b8-c0fa-13298abfd51c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.7-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.4/221.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.2-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.2 openai-1.3.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import openai"
      ],
      "metadata": {
        "id": "-VxsDGDHVHku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "def remove_duplicates_and_extract(json_directory, output_file_path):\n",
        "    unique_conversations = []\n",
        "\n",
        "    for filename in os.listdir(json_directory):\n",
        "        if filename.endswith('.json'):\n",
        "            file_path = os.path.join(json_directory, filename)\n",
        "\n",
        "            with open(file_path, 'r') as json_file:\n",
        "                data = json.load(json_file)\n",
        "\n",
        "            for source in data.get('Sources', []):\n",
        "                for chat in source.get('ChatgptSharing', []):\n",
        "                    if \"NumberOfPrompts\" in chat and chat[\"NumberOfPrompts\"] == 1:\n",
        "                        prompt = chat[\"Conversations\"][0].get(\"Prompt\", \"\")\n",
        "                        answer = chat[\"Conversations\"][0].get(\"Answer\", \"\")\n",
        "                        if \"[CODE_BLOCK_0]\" not in answer:\n",
        "                            conversation = {\"Prompt\": prompt, \"Answer\": answer}\n",
        "                            if conversation not in unique_conversations:\n",
        "                                unique_conversations.append(conversation)\n",
        "\n",
        "    with open(output_file_path, 'w') as json_file:\n",
        "        json.dump(unique_conversations, json_file, indent=4)\n",
        "\n",
        "    print(f\"All unique prompts and answers saved to {output_file_path}\")\n",
        "\n",
        "# Usage\n",
        "json_directory = '/content/drive/MyDrive/DevGPT/snapshot_20230831'\n",
        "output_file_path = \"/content/drive/MyDrive/SE_Project/samp6_unique.json\"\n",
        "remove_duplicates_and_extract(json_directory, output_file_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqbADkyx-ybE",
        "outputId": "32372d81-8de2-45a3-e31b-f86a6589d359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All unique prompts and answers saved to /content/drive/MyDrive/SE_Project/samp6_unique.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "\n",
        "def contains_chinese_or_japanese(text):\n",
        "    # Use regular expression to check if the text contains Chinese or Japanese characters\n",
        "    chinese_japanese_pattern = re.compile('[\\u4e00-\\u9fa5ぁ-んァ-ヶ]')\n",
        "    return bool(chinese_japanese_pattern.search(text))\n",
        "\n",
        "def preprocess_json(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
        "        data = json.load(infile)\n",
        "\n",
        "        # Filter out entries where either 'Prompt' or 'Answer' contains Chinese or Japanese characters\n",
        "        filtered_data = [item for item in data if not (contains_chinese_or_japanese(item['Prompt']) or contains_chinese_or_japanese(item['Answer']))]\n",
        "\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        json.dump(filtered_data, outfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file_path = '/content/drive/MyDrive/SE_Project/samp6_unique.json'  # Path to the uploaded JSON file\n",
        "    output_file_path = '/content/drive/MyDrive/SE_Project/samp6_final.json'\n",
        "\n",
        "    preprocess_json(input_file_path, output_file_path)\n"
      ],
      "metadata": {
        "id": "5BRdNuvVc4pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "\n",
        "# Set your OpenAI API key\n",
        "openai.api_key = 'OPENAI_KEY'\n",
        "\n",
        "# Read prompts from JSON file\n",
        "with open('/content/drive/MyDrive/SE_Project/samp6_unique.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "# List to store new responses\n",
        "new_responses = []\n",
        "\n",
        "# Loop through each prompt and make API calls\n",
        "for item in data:\n",
        "    prompt = item[\"Prompt\"]\n",
        "\n",
        "    try:\n",
        "        response = openai.Completion.create(\n",
        "            model=\"gpt-3.5-turbo-1106\",  # Updated model\n",
        "            prompt=prompt,\n",
        "            max_tokens=1500  # You can adjust this parameter based on the desired response length\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating response for prompt: {prompt}\")\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "    # Extract the generated response\n",
        "    generated_response = response['choices'][0]['text'].strip()\n",
        "\n",
        "    # Store the new response in the same format\n",
        "    new_responses.append({\n",
        "        \"Prompt\": prompt,\n",
        "        \"Answer\": generated_response\n",
        "    })\n",
        "\n",
        "# Save the new_responses to a JSON file\n",
        "output_file_path = \"/content/drive/MyDrive/SE_Project/samp6_unique_openai.json\"\n",
        "with open(output_file_path, 'w') as file:\n",
        "    json.dump(new_responses, file, indent=4)\n",
        "\n",
        "print(f\"New responses saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "OKveM6mSDqM-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}